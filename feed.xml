<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://cymtrick.github.io/al-folio/feed.xml" rel="self" type="application/atom+xml"/><link href="https://cymtrick.github.io/al-folio/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-21T04:16:44+00:00</updated><id>https://cymtrick.github.io/al-folio/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Fp8 To Fp16 Gemm Amd</title><link href="https://cymtrick.github.io/al-folio/blog/2025/fp8-to-fp16-gemm-amd/" rel="alternate" type="text/html" title="Fp8 To Fp16 Gemm Amd"/><published>2025-04-20T00:00:00+00:00</published><updated>2025-04-20T00:00:00+00:00</updated><id>https://cymtrick.github.io/al-folio/blog/2025/fp8-to-fp16-gemm-amd</id><content type="html" xml:base="https://cymtrick.github.io/al-folio/blog/2025/fp8-to-fp16-gemm-amd/"><![CDATA[<h1 id="optimizing-fp8-to-bf16-gemm-on-amd-mi300-hip-only">Optimizing FP8-to-BF16 GEMM on AMD MI300 (HIP-Only)</h1> <h3 id="we-will-apply-several-optimization-strategies-each-targeting-a-specific-aspect-of-the-computation">We will apply several optimization strategies, each targeting a specific aspect of the computation:</h3> <p><img src="al-folio/assets/img/Screenshot_2025-04-18_at_7.15.25_PM.png" alt="Screenshot 2025-04-18 at 7.15.25 PM.png"/></p> <ul> <li><strong>Register tiling</strong> to compute a tile of C per thread, reusing values in fast registers.</li> <li><strong>Vectorized FP8 loads</strong> (with FP32 unpacking and scaling) to fully utilize memory bandwidth.</li> <li><strong>Shared Memory (LDS) tiling</strong> for FP8 A and B sub-tiles, with proper padding to avoid bank conflicts and double-buffering to overlap memory operations with computation.</li> <li><strong>Wavefront-level reductions</strong> using AMD-friendly intrinsics (e.g. DPP shuffle) to sum partial results across lanes if needed.</li> <li><strong>Memory hierarchy optimizations</strong> to minimize costly global memory access by maximizing on-chip (LDS/register) reuse.</li> <li><strong>Instruction pipeline scheduling</strong> (software pipelining) to hide memory latency by prefetching the next K-block while computing the current block.</li> <li><strong>Specialization for small K (e.g. K=256)</strong> to handle memory-bound cases differently from compute-bound cases.</li> <li><strong>Compiler and launch configuration tweaks</strong> to ensure the GPU executes the kernel at optimal wavefront size, uses fast math approximations, and achieves high occupancy or the best occupancy-performance balance.</li> </ul> <p>We detail each of these steps below, including the implementation approach and the rationale, and finally estimate the performance impact of each optimization.</p> <p><img src="al-folio/assets/img/acb33744-23e0-4865-b98e-ba548bedeead.png" alt="acb33744-23e0-4865-b98e-ba548bedeead.png"/></p> <p><em>Figure: The AMD GCN Compute Unit (CU) architecture – each CU has four 16-lane SIMD units and a large 256 KB vector register file (VGPR) split into 64 KB per SIMD. Each thread (lane) can use up to 256 32-bit registers <a href="https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf#:~:text=%E2%81%83%204x%20SIMD%20vector%20units,byte%20entries">olcf.ornl.gov</a>, enabling aggressive register blocking. We leverage this architecture to optimize FP8-BF16 GEMM.</em></p> <h2 id="register-tiling-for-output-c">Register Tiling for Output C</h2> <p>To maximize data reuse, each thread in a HIP kernel should compute a small <strong>tile of C</strong> held in registers, instead of computing just one C element. By accumulating a tile of outputs per thread, we amortize the cost of loading A and B values over multiple FMA (fused multiply-add) operations. On AMD MI300, the register file is large (up to 256 FP32 registers per thread lane) <a href="https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf#:~:text=%E2%81%83%204x%20SIMD%20vector%20units,byte%20entries">olcf.ornl.gov</a>, which allows each thread to safely keep several partial sums and input values in registers without spilling to memory. This <strong>register tiling</strong> approach dramatically reduces redundant memory accesses:</p> <ul> <li><strong>Thread output tile size:</strong> We choose a tile size (e.g. 4×4 or 2×8 outputs per thread) such that the required registers fit within the 256 VGPR limit. For example, a 4×4 tile of BF16 outputs requires 16 accumulators (32-bit each for accumulation in FP32), plus some registers for loaded A/B data. This is well within the per-thread budget on MI300. Using more registers (larger tile) can further increase data reuse but may reduce occupancy, so we balance tile dimensions to keep enough threads active.</li> <li><strong>Data reuse via outer-product:</strong> With a tile per thread, we can use each loaded value multiple times. For instance, if a thread’s tile is 4 outputs across 4 columns of C in the same row (M), the thread can load one A value (one row, one K index) and four B values (for four columns) and multiply-accumulate them into its 4 output registers. In the next step, it loads the next A value (same row) and another set of 4 B values, accumulating into the same outputs. This effectively performs 4 FMAs per pair of loads (one A, four B’s), boosting the arithmetic intensity.</li> <li><strong>Warp-level tiling:</strong> Threads are grouped in wavefronts (warps) of 64 on AMD GCN hardware <a href="https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf#:~:text=%E2%96%AA%20AMD%20GCN%20hardware%20%E2%80%98warp%E2%80%99,addressing%20is%20enabled%20by%20default">olcf.ornl.gov</a>. We map each warp to a larger C submatrix (e.g. 64×64 output tile per warp, divided into 64 thread-tiles of 8×1 or 4×2 each). All threads in a wave cooperate, with each thread handling a distinct portion of the output. This organization ensures that the entire warp works on a contiguous C tile, facilitating coalesced memory accesses for both A and B, as described later.</li> </ul> <p>By blocking computation into per-thread tiles in registers, we minimize writing to global memory (only once, after full accumulation) and re-use each loaded operand across multiple multiply-adds. <strong>This substantially increases arithmetic intensity</strong>, meaning more math operations per byte of data fetched <a href="https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf#:~:text=%E2%81%83%20Access%20is%20faster%20than,All%20rights%20reserved">olcf.ornl.gov</a><a href="https://rocm.blogs.amd.com/software-tools-optimization/amdgcn-isa/README.html#:~:text=Registers%20are%20high%20speed%20memory,and%20data%20is%20stored%20out">rocm.blogs.amd.com</a>. As a result, the GPU spends more time doing FMA (which it can do at petaflop rates) and less time stalled on memory. Register tiling is the foundation for approaching the provided 8.63 µs–155.30 µs latency targets.</p> <p><img src="al-folio/assets/img/Screenshot_2025-04-20_at_9.02.40_PM.png" alt="Screenshot 2025-04-20 at 9.02.40 PM.png"/></p> <h2 id="shared-memory-tiling-and-double-buffering">Shared Memory Tiling and Double-Buffering</h2> <p>After fetching FP8 data from global memory, we stage it in <strong>shared memory</strong> (called <em>LDS</em> on AMD GPUs <a href="https://rocm.blogs.amd.com/software-tools-optimization/amdgcn-isa/README.html#:~:text=,as%20an%20explicitly%20managed%20cache">rocm.blogs.amd.com</a>) for reuse by all threads in the block. Shared memory acts as a manually managed cache, enabling fast access to the same data from multiple threads. We employ tiling in shared memory and double-buffering to optimize usage:</p> <ul> <li><strong>Tiling in LDS:</strong> We partition the K dimension into sub-tiles (e.g. K_tile = 256 or 512) that fit in shared memory. For each such tile, we load a block of matrix A (dimensions: M_tile × K_tile) and matrix B (N_tile × K_tile) into shared memory. For example, if a thread block covers 128×128 of the output (M_tile=128, N_tile=128), and we choose K_tile=256, then A_tile size is 128×256 and B_tile is 128×256 (B is effectively transposed when loaded for convenience). All threads in the block cooperate: each thread reads a few FP8 elements from global and writes them to LDS. Because we arranged global loads coalesced, each warp brings in a contiguous chunk for A or B. Once in shared memory, values can be read by any thread in the block at <strong>LDS speed, which is much faster than global memory (though slightly slower than registers)<a href="https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf#:~:text=%E2%81%83%20Access%20is%20faster%20than,All%20rights%20reserved">olcf.ornl.gov</a></strong>. This allows each element of A and B to be reused by dozens of FMAs without reloading from DRAM.</li> <li><strong>Padding to avoid bank conflicts:</strong> AMD’s shared memory is banked (32 banks on GCN GPUs)<a href="https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf#:~:text=%E2%81%83%2032%20banks%20with%20conflict,%E2%80%93%20shared%20by%20all%20CUs">olcf.ornl.gov</a>. We pad the shared arrays so that threads accessing A or B simultaneously do not contend for the same bank. For instance, if each thread reads a 32-bit FP32 value (after unpack) from shared memory, addresses that are multiples of 32 words might lie in the same bank. By adding a slight padding (e.g. an extra column) in the LDS tile for A and B, we ensure aligned, conflict-free access patterns. This way, all 32 or 64 threads in a wave can read their data in parallel without serialization due to bank conflicts.</li> <li><strong>Double-buffering (ping-pong buffers):</strong> To <strong>hide global memory latency</strong>, we use two buffers in shared memory for each matrix. While the threads are computing on the current tile (performing FMAs using data in LDS), we concurrently prefetch the next tile of A and B into the alternate buffer (if the hardware and HIP allow overlapping global memory operations with computation). The typical sequence is: <ol> <li>Load A_tile0 and B_tile0 into shared memory (buffer 0).</li> <li>__syncthreads(); Compute on tile0 (all threads read from shared buffer 0, do FMAs into registers).</li> <li>While computing tile0, begin loading A_tile1, B_tile1 into buffer 1 (asynchronous or by carefully placing the load instructions before the compute loop for tile0 finishes).</li> <li>__syncthreads(); Switch to compute on tile1 (now data is in buffer 1) while prefetching next tile into buffer 0.</li> <li>Repeat until all K tiles are done.</li> </ol> <p>This overlapping effectively creates a software pipeline. Memory fetching for the next segment of K runs in parallel with arithmetic on the current segment. By the time compute on tile <code class="language-plaintext highlighter-rouge">n</code> completes, tile <code class="language-plaintext highlighter-rouge">n+1</code> is ready in LDS. This hides the latency of global memory and ensures the compute units stay busy almost continuously<a href="https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf#:~:text=%E2%81%83%20Access%20is%20faster%20than,All%20rights%20reserved">olcf.ornl.gov</a>.</p> </li> <li><strong>Thread block scheduling:</strong> We size the shared memory tiles and thread block such that the entire tile fits in LDS (MI300 offers large shared memory per CU, likely 64 KB or more). We also consider occupancy: double-buffering means at least 2× tile data is in LDS at once, but it allows us to keep the pipeline full. Even if using more shared memory per block reduces occupancy (fewer blocks resident at once), the latency hiding from double-buffering and the increased data reuse often more than compensate.</li> </ul> <p>Using shared memory tiles with double-buffering dramatically reduces the required bandwidth to global memory. Each FP8 element from A or B is fetched from DRAM only once per tile, then reused from LDS many times (on the order of M_tile or N_tile times, respectively). With double-buffering, we approach ideal usage of the memory system: global memory is almost always being loaded in the background, and the compute units are nearly 100% busy doing FMA on the foreground tile. This is essential to reach the target latencies, especially for large K where memory access would otherwise be a bottleneck.</p> <h2 id="wave-level-reductions-with-amd-dpp-and-shuffles">Wave-Level Reductions with AMD DPP and Shuffles</h2> <p>On some GPU architectures, partial results need to be reduced across threads. In our GEMM approach, each thread is responsible for a unique C tile, so <strong>most of the accumulation is done in registers locally</strong>, not requiring cross-thread reduction. However, there are scenarios where wave-level reduction techniques become useful:</p> <ul> <li>If we <strong>split the K dimension across multiple warps or thread blocks</strong> (so-called split-K parallelism) to reduce per-thread workload, then partial sums from different warps must be added together. This could be needed if K is extremely large or if using specialized matrix instructions (MFMA) that accumulate internally.</li> <li>If a warp of 64 threads collectively computes one large output tile, they might each compute a partial sum of an output element, then combine them.</li> </ul> <p>For AMD GCN GPUs, we have <strong>wavefront (warp) size = 64 threads</strong> by default<a href="https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf#:~:text=%E2%96%AA%20AMD%20GCN%20hardware%20%E2%80%98warp%E2%80%99,addressing%20is%20enabled%20by%20default">olcf.ornl.gov</a> (RDNA-based GPUs can use wave32 or wave64). AMD provides efficient intrinsics for cross-lane communication:</p> <ul> <li><strong>DPP (Data Parallel Primitives):</strong> DPP instructions allow permuting vector register values between lanes in the same wavefront with a single instruction (with patterns like rotate, swap, etc.) and optionally combining with an ALU operation. We can use DPP to implement a warp reduction in logarithmic steps. For example, to reduce a value across 64 lanes, we can use a sequence of DPP operations that shuffle values by fixed offsets (like 1,2,4,8,… positions) while performing add operations. This is done entirely in registers without using shared memory, making it very fast. Although HIP C++ does not expose DPP directly, the compiler may provide built-ins or map high-level warp shuffle calls to DPP patterns on GCN. An alternative is using inline assembly, but since we avoid that, we rely on provided intrinsics or optimize the algorithm to minimize need for these operations.</li> <li><strong>Wave shuffles (ballot and shuffle):</strong> Modern HIP includes warp shuffle functions (similar to CUDA’s <code class="language-plaintext highlighter-rouge">__shfl_xor</code>, <code class="language-plaintext highlighter-rouge">__shfl_down</code>, etc.) that on AMD will utilize the GPU’s ds_swizzle or other cross-lane capabilities. Using <code class="language-plaintext highlighter-rouge">__shfl_down_sync</code> for example, threads can pass values to one another. If DPP is not directly accessible, we can still achieve reductions by iterative shuffle: e.g., each thread adds the value from a partner thread (e.g. thread <code class="language-plaintext highlighter-rouge">t</code> reads value from <code class="language-plaintext highlighter-rouge">t+32</code>, then <code class="language-plaintext highlighter-rouge">t+16</code>, etc. in a loop). Because AMD wavefront is 64, it will take 6 iterations (for 64 lanes) instead of 5 (for 32 lanes on NVIDIA), but the principle is the same.</li> <li><strong>SIMD-wave intrinsic optimization:</strong> On AMD, an alternative to explicit shuffle is to use wave-wide prefix sum or reduction library calls, or even to use the SALU (scalar ALU) for uniform operations if applicable. If our partial sums to reduce are arranged such that each wave’s lanes that need to be summed are contiguous, we could have one lane compute the sum of some values by reading them from neighboring lanes via a special read-first-lane instruction or by writing to LDS and reading by one lane. However, these are generally slower than using vector ops like DPP.</li> </ul> <p>In the context of our GEMM, we minimize the need for cross-thread reduction by designing the kernel so that each output element is computed by a single thread. Still, in edge cases or certain tiling strategies, wave-level reductions can sum contributions efficiently without leaving the wave. Leveraging these <strong>AMD-friendly idioms</strong> (especially DPP, which is known to be very efficient on GCN for reductions) can save on extra memory operations. For example, rather than each thread storing partial results to shared memory and a designated thread reading them to sum (which would be slow), a DPP add can do it in registers in one or two cycles. If our kernel needed to sum partial C from multiple phases or from multiple split-K slices, a couple of wave-level shuffles would complete the sum with negligible overhead.</p> <p>In summary, while our primary strategy is to give each thread its own outputs (avoiding heavy cross-lane reductions), AMD’s wavefront collectives are available to combine partial results as needed. This ensures that any necessary reductions are done <strong>in-register across the 64 lanes</strong> with minimal synchronization cost.</p> <h2 id="memory-hierarchy-optimizations-global--shared--register">Memory Hierarchy Optimizations (Global → Shared → Register)</h2> <p><img src="al-folio/assets/img/Screenshot_2025-04-20_at_9.01.26_PM.png" alt="Screenshot 2025-04-20 at 9.01.26 PM.png"/></p> <p>Optimizing data movement through the memory hierarchy is crucial for performance. We intentionally stage data in the <strong>global → shared (LDS) → register</strong> hierarchy to maximize reuse and bandwidth at each level:</p> <ul> <li><strong>Global Memory (HBM/DDR)</strong>: This is the slowest (high latency) but highest capacity level. We minimize accesses here by reading each matrix element at most once. All FP8 A and B elements are read from global memory, coalesced and in vectorized chunks as described, then reused heavily from faster memories. We also ensure that global stores of the BF16 results are coalesced: each thread writes its few output values contiguously, and threads collectively write a full aligned 128B segment of C at a time to achieve full throughput.</li> <li><strong>L2 and L1 caches</strong>: The MI300 GPU likely has an L2 cache and possibly a L1 data cache for global memory (16 KiB per CU as per GCN architecture)<a href="https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf#:~:text=%E2%81%83%2032%20banks%20with%20conflict,%E2%80%93%20shared%20by%20all%20CUs">olcf.ornl.gov</a>. Our access pattern (coalesced and streaming) will naturally use the caches effectively for any repeated accesses. In GEMM, however, each element of A and B is typically used once per kernel execution (because we tile K), so caching of global memory is less critical than tiling in shared memory. Still, the writes to C (BF16) will benefit from write-combining in cache. We also rely on the cache to handle any memory bank differences between global and LDS.</li> <li><strong>Shared Memory (LDS)</strong>: This is the on-chip user-managed scratchpad that we explicitly use for blocking. Its latency is an order of magnitude lower than global memory, and it allows <strong>many threads to access data in parallel</strong> without congesting global memory<a href="https://rocm.blogs.amd.com/software-tools-optimization/amdgcn-isa/README.html#:~:text=,as%20an%20explicitly%20managed%20cache">rocm.blogs.amd.com</a>. By loading each tile into LDS, we effectively <strong>convert random global accesses into structured on-chip accesses</strong>. Within a tile, threads access A and B from LDS in a pattern that the compiler can schedule very efficiently (often, it will map LDS loads to on-chip operations that take only a few cycles). We take advantage of this by loading large tiles (subject to occupancy limits) so that each global load yields hundreds or thousands of on-chip accesses (each multiply using the data is an “access” to the operand).</li> <li><strong>Registers (VGPR/AGPR)</strong>: The final level is the registers, which have the fastest access (single cycle). All critical innermost computations occur here. After a value is loaded from shared memory into a register, it is reused for multiple FMAs (for example, a loaded B value in a register might be multiplied by 4 different A values in succession by the same thread, producing 4 updates to outputs). The output accumulations themselves remain in registers until the end, so there is no spilling to memory mid-computation. According to AMD optimization guidelines, “almost all computation uses registers” for precisely this reason <a href="https://rocm.blogs.amd.com/software-tools-optimization/amdgcn-isa/README.html#:~:text=Registers%20are%20high%20speed%20memory,and%20data%20is%20stored%20out">rocm.blogs.amd.com</a>. Using registers for accumulation and operand reuse allows us to achieve the theoretical FMA throughput of the hardware.</li> </ul> <p>By <strong>orchestrating data movement</strong> in this way, we maximize bandwidth utilization at each level: Global memory bandwidth is saved by only reading each value once; the LDS is used to serve data at high speed to many threads (and thanks to careful padding, without bank conflicts); registers are used to their fullest extent to keep data close to the ALUs. The net effect is that the kernel approaches the roofline limit: the compute units are fed with data at the rate needed to keep them busy, hitting the peak FLOPS for FP8×FP8→BF16 accumulation.</p> <h2 id="pipeline-scheduling-to-hide-memory-latency">Pipeline Scheduling to Hide Memory Latency</h2> <p>To achieve overlap of computation and memory transfers, we implement <strong>software pipelining</strong> in the kernel. The concept is to reorganize instructions so that while one set of operations is waiting on memory, another independent set of operations is executed, thereby hiding the latency. In our GEMM kernel, this is realized through the double-buffering mechanism and careful scheduling:</p> <ul> <li><strong>Interleaving loads and computes:</strong> We issue asynchronous global loads for the next tile well <em>before</em> finishing the computation on the current tile. In practice, we structure the kernel as a loop over K-tiles, where each iteration does: <ol> <li>Issue global loads for A_tile and B_tile of the <em>next</em> iteration (except in the very last iteration) and store them into the alternate shared memory buffer.</li> <li>Compute using the data from the <em>current</em> tile (already in shared memory from the previous iteration).</li> <li>Synchronize to ensure loads are completed before the next iteration.</li> </ol> <p>By moving the load instructions to earlier in the loop body (relative to the use of the data), we create an effective <strong>prefetch</strong>. The compiler will see that the loads of tile <code class="language-plaintext highlighter-rouge">n+1</code> are not dependent on the compute of tile <code class="language-plaintext highlighter-rouge">n</code>, and if we mark them with the proper <code class="language-plaintext highlighter-rouge">__syncthreads()</code> ordering, it can overlap those memory operations with the ongoing ALU operations.</p> </li> <li><strong>Latency hiding:</strong> The latency to fetch data from global memory (even from HBM, which might be hundreds of cycles) is thus covered by useful computation. As long as the compute on each tile takes a comparable amount of time to the load of the next tile, the global memory latency will be fully hidden. We can tune tile sizes such that this balance is achieved. For example, if computation on a 256×K tile takes ~1000 cycles and memory latency is ~400 cycles, we are safely covering the latency. If needed, we can enlarge the tile (do more computation per tile) to increase compute time, or use more threads (which increases parallel operations per tile load).</li> <li><strong>Prefetch depth:</strong> We primarily prefetch one tile ahead. MI300’s large caches and multiple memory queues might even allow prefetching more than one tile ahead, but one tile double-buffering is usually sufficient. We also ensure that the compiler does not reorder the loads and compute incorrectly; the use of barriers (<code class="language-plaintext highlighter-rouge">__syncthreads()</code>) as synchronization points naturally divides the schedule into stages, preventing hazards.</li> <li><strong>ILP within a tile:</strong> In addition to overlapping different tile loads, we also increase <strong>instruction-level parallelism</strong> within the tile’s compute. Each thread can unroll the inner loop (over K elements within the tile) and perform multiple independent FMAs before needing the next loaded value. Modern compilers and GPUs can dual-issue or hide pipeline latency by having independent instructions ready to execute. By unrolling a bit or having each thread compute multiple outputs, we create independent sequences the hardware can intermix. This means even the latency of a shared memory load or a dependent chain is partly hidden by other independent operations on the ALUs.</li> </ul> <p>The result of these scheduling techniques is that both global and shared memory latencies are largely hidden under useful work. The kernel’s execution becomes <strong>compute-bound</strong> rather than stall-bound: as soon as one tile’s worth of FMAs are done, the next tile’s data is already available, so the work flows steadily. This is critical for hitting the sub-10µs execution times on smaller sizes and keeping the larger cases (e.g. 6144×4608×7168) close to peak utilization without pauses. Essentially, we create a steady-state pipeline from memory to ALU: at any given moment, one tile is being loaded, one is being computed, and perhaps one is being written back (for C output, though final write-back is relatively minor and can overlap too). Such overlapping can improve overall performance by a significant factor – without it, the GPU would frequently idle waiting for data.</p> <h2 id="specialization-for-short-k-cases-k--256">Specialization for Short-K Cases (K = 256)</h2> <p>When the K dimension is very small (256 in our benchmark cases), the problem characteristics change. With K=256, each output involves far fewer multiplications, so the computation is faster and the workload is more memory-bound (lower arithmetic intensity). To handle this efficiently, we introduce specialized strategies for <strong>short-K scenarios</strong>:</p> <ul> <li><strong>Larger thread blocks covering more of M×N:</strong> With a small K, each thread’s computation (even for a decent tile size) is relatively short. We can afford to increase the tile sizes in M and N to involve more threads and cover a larger portion of the output matrix in one go. For example, for K=256 we might let each thread block compute a 256×256 chunk of C (as opposed to perhaps 128×128 for K=7168). This means each block will load more A and B data (which is still manageable since K is small, the total LDS usage = 256×256 FP8 for A + B, which is not huge) and produce more output. The advantage is twofold: (a) we get more reuse of each loaded A/B element (now each element might be used to produce 256 results instead of 128, improving flops/byte), and (b) we launch fewer blocks overall, reducing kernel launch and scheduling overhead.</li> <li><strong>Fewer tiles (or a single tile) in K dimension:</strong> If K=256, we could even make the entire K a single tile. That is, the inner loop that iterates over K tiles becomes trivial – we load 256 once and compute. This eliminates the overhead of the loop and any double-buffering (since it’s not needed – we can load everything once). The whole operation becomes one bulk matrix multiplication of a tile with K fully unrolled. This is extremely efficient because it removes loop bookkeeping and extra synchronization. We will ensure that the code path for small K simply loads A and B into shared memory (or even directly into registers if feasible) in one shot, and then performs the FMAs in an unrolled loop.</li> <li><strong>Maximizing parallelism</strong>: With fewer arithmetic operations to perform, it’s important to keep all SMs (CUs) on the GPU busy. We may increase the number of thread blocks (by using smaller blocks covering smaller M×N tiles) to occupy all compute units, even if each block finishes quickly. Essentially, the kernel launch configuration for K=256 might use more blocks with less work each, to ensure latency is minimized. However, we must balance this with the benefit of larger tiles – so there’s a sweet spot. In practice, for sizes like 1024×7168×256, the M and N are large enough to have many blocks even with large tile sizes. For 6144×7168×256, we can still tile in a way that we have say 24 blocks in M and 28 in N or similar to fill the GPU.</li> </ul> <p>The short-K specialization is justified by the roofline model: a smaller K means a lower computational intensity, so the kernel tends to be memory-bandwidth-limited. We adapt by <strong>reducing overhead and enhancing parallelism</strong>. These special-case kernels for K=256 may achieve better efficiency (in TFLOPs) than a one-size-fits-all kernel that is geared toward large K. For instance, our baseline “speed-of-light” numbers show the FP8 GEMM achieving ~1300 TFLOPs for large K, but only ~600 TFLOPs for the 256 K cases. Our specialized optimizations aim to close that gap by boosting memory efficiency (through larger tiles and one-shot loading) and ensuring full device occupancy. As a result, we expect the K=256 cases to see a significant speedup, possibly approaching the performance of large-K cases (maybe 1.5–2× faster than they would be with the generic kernel). This helps us meet or beat the 3.17 µs and 17.27 µs targets for those short-K shapes.</p> <h2 id="compiler-flags-and-launch-configuration-tweaks">Compiler Flags and Launch Configuration Tweaks</h2> <p>Even with a well-crafted kernel, certain compiler options and launch parameters can impact performance on MI300. We employ these tweaks to squeeze out the last bit of performance:</p> <ul> <li><strong>Wavefront size selection:</strong> AMD’s RDNA and CDNA architectures can support wavefront sizes of 32 or 64. By default, GCN (like MI250) uses 64 threads per wave<a href="https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf#:~:text=%E2%96%AA%20AMD%20GCN%20hardware%20%E2%80%98warp%E2%80%99,addressing%20is%20enabled%20by%20default">olcf.ornl.gov</a>. We will experiment with <code class="language-plaintext highlighter-rouge">mwavefrontsize64</code> (and 32) to see which yields better performance <a href="https://rocmdocs.amd.com/projects/llvm-project/en/latest/reference/rocmcc.html#:~:text=%60">rocmdocs.amd.com</a>. A wavefront of 64 maximizes vector utilization (all 64 ALUs in a SIMD unit), which is generally best for dense GEMM. However, a wave32 could allow higher occupancy or better scheduling on RDNA-based GPUs. If MI300’s GPU is CDNA3 with fixed wave64, we stick to 64. But if it can use 32, we might try wave32 for the short-K kernel to increase the number of warps (thus more parallel loads) per CU. In any case, we explicitly set the wavefront size to avoid any ambiguity, and ensure our code doesn’t assume a warp of 32 (a common NVIDIA assumption)<a href="https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf#:~:text=%E2%80%A2%20Hard,register%20file%20on%20NVIDIA%20hardware">olcf.ornl.gov</a>.</li> <li><strong>Fast math and precision flags:</strong> We compile with <code class="language-plaintext highlighter-rouge">O3 -ffast-math</code> to allow aggressive optimization of floating-point calculations <a href="https://rocmdocs.amd.com/projects/llvm-project/en/latest/reference/rocmcc.html#:~:text=%60">rocmdocs.amd.com</a>. Fast math will enable fused multiply-add (which we want – though BF16/FP32 FMAs are usually fused anyway) and might let the compiler re-associate operations. Since we handle scaling and conversion carefully, we ensure these transformations won’t break our algorithm. We also enable any available flags for denormal flushing (e4m3 doesn’t really have subnormals, and BF16 has limited range, so flush-to-zero is fine). Fast math can improve throughput by a few percent by using approximate math functions (not heavily used in GEMM) and by relaxing IEEE compliance, which is acceptable here.</li> <li><strong>Bfloat16 and FP8 support intrinsics:</strong> We use built-in types for BF16 and potentially FP8 (if provided). For example, some compilers have <code class="language-plaintext highlighter-rouge">__bf16</code> type or <code class="language-plaintext highlighter-rouge">_Float16</code> that maps to BF16 on AMD. Using these can sometimes allow the compiler to use special hardware instructions (like packed BF16 stores or matrix multiply-accumulate units). If MI300 supports WMMA-like instructions for FP8×FP8→BF16, the compiler might auto-generate them when using specific types or intrinsics. We will ensure our code is ready to take advantage by using the appropriate data types and letting the compiler schedule those operations.</li> <li><strong>Occupancy tuning:</strong> We will launch the kernel with an optimal number of threads per block (work-group size) that balances resource usage. For instance, a 256-thread block (4 warps of 64) is common for GEMM. We ensure this count divides evenly into our tile sizes. We might also use the HIP occupancy API or launch bounds attributes to make sure we don’t launch more blocks per CU than ideal. For example, if each block uses a lot of shared memory, the GPU may only allow a few blocks resident at once. We might explicitly limit to, say, 2 blocks per CU (occupancy 50%) if that avoids LDS capacity over-subscription, because having more would just cause spills or LDS thrashing. Past research has shown that highest occupancy isn’t always best for GEMM – sometimes fewer blocks (lower occupancy) but with more registers and shared memory per block yields better performance<a href="https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf#:~:text=%E2%80%A2%20Hard,register%20file%20on%20NVIDIA%20hardware">olcf.ornl.gov</a>. We use those insights here.</li> <li><strong>Unroll and pipeline pragma hints:</strong> We can use <code class="language-plaintext highlighter-rouge">#pragma unroll</code> for the inner loop over K to ensure the compiler unrolls it sufficiently (especially for K=256 case where we might unroll fully). We can also use <code class="language-plaintext highlighter-rouge">__syncthreads()</code> placement and perhaps <code class="language-plaintext highlighter-rouge">__builtin_amdgcn_s_barrier()</code> to fine-tune synchronization points. If available, we might use the new asynchronous copy intrinsic from global to shared (HIP.maybe has something like <code class="language-plaintext highlighter-rouge">llvm.amdgcn.raw.buffer.load</code> we can invoke) to better overlap loads. All such tweaks are done at compile time without writing assembly by hand.</li> <li><strong>Vectorization and alignment hints:</strong> We align arrays to 128-bit boundaries (using <code class="language-plaintext highlighter-rouge">__align__</code> keywords on shared memory buffers and appropriate pitch for global memory) to help the compiler generate coalesced accesses. We might also use the <code class="language-plaintext highlighter-rouge">restrict</code> keyword on pointers to A, B, C to help the compiler optimize loads/stores (guaranteeing no alias). Additionally, setting launch parameters so that <code class="language-plaintext highlighter-rouge">N</code> is a multiple of the warp size ensures no partial warps accessing out-of-bounds, etc.</li> </ul> <p>These compiler and configuration optimizations ensure that our kernel runs at full throttle. For example, forcing wave64 and using fast-math can directly increase throughput by enabling more ILP and better FMA scheduling <a href="https://rocmdocs.amd.com/projects/llvm-project/en/latest/reference/rocmcc.html#:~:text=%60">rocmdocs.amd.com</a>. Proper block sizing avoids occupancy pitfalls and ensures we fully utilize the GPU’s CUs. In sum, we tune the environment such that nothing prevents the hardware from achieving the performance we expect from our algorithmic optimizations.</p> <p>##</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Optimizing FP8-to-BF16 GEMM on AMD MI300 (HIP-Only)]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://cymtrick.github.io/al-folio/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://cymtrick.github.io/al-folio/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://cymtrick.github.io/al-folio/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024[[read-time]] min read We’re introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we’re introducing Gemini 1.5 Flash: a model that’s lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We’re also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5’s 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it’s a lighter weight model than 1.5 Pro, it’s highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it’s been trained by 1.5 Pro through a process called “distillation,” where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash’s availability and pricing.Over the last few months, we’ve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we’ve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We’ve improved control over the model’s responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we’ve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we’re now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do — not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we’re also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We’re announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we’ve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind’s mission to build AI responsibly to benefit humanity, we’ve always wanted to develop universal AI agents that can be helpful in everyday life. That’s why today, we’re sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do — and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we’ve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we’ve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we’ve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they’re being used in, and respond quickly, in conversation.With technology like this, it’s easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We’ve made incredible progress so far with our family of Gemini models, and we’re always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we’re able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google’s privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let’s stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://cymtrick.github.io/al-folio/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://cymtrick.github.io/al-folio/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://cymtrick.github.io/al-folio/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[<h3>External Posts on Your al-folio Blog</h3> <p>If you prefer publishing blog posts on medium.com or other external sources, starting version v0.5.0, <a href="https://github.com/alshedivat/al-folio">al-folio</a> lets you to display your external posts in the blog feed of your website! 🎉🎉</p> <p>Configuring external sources of super simple. After upgrading to v0.5.0, just add the following section to your _config.yml:</p> <pre>external_sources:<br />  - name: medium.com  # name of the source (arbitrary string)<br />    rss_url: <a href="https://medium.com/@al-folio/feed">https://medium.com/@&lt;your-medium-username&gt;/feed</a></pre> <p>The example above adds your medium.com blog post feed as an external source. But you can add arbitrary RSS feeds as sources.</p> <p>Any questions or suggestions? 👉 Start <a href="https://github.com/alshedivat/al-folio/discussions">a discussion on GitHub</a>!</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b60a1d241a0a" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>