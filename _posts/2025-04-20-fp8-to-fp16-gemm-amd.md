# Optimizing FP8-to-BF16 GEMM on AMD MI300 (HIP-Only)

### We will apply several optimization strategies, each targeting a specific aspect of the computation:

![Screenshot 2025-04-18 at 7.15.25 PM.png](al-folio/assets/img/Screenshot_2025-04-18_at_7.15.25_PM.png)

- **Register tiling** to compute a tile of C per thread, reusing values in fast registers.
- **Vectorized FP8 loads** (with FP32 unpacking and scaling) to fully utilize memory bandwidth.
- **Shared Memory (LDS) tiling** for FP8 A and B sub-tiles, with proper padding to avoid bank conflicts and double-buffering to overlap memory operations with computation.
- **Wavefront-level reductions** using AMD-friendly intrinsics (e.g. DPP shuffle) to sum partial results across lanes if needed.
- **Memory hierarchy optimizations** to minimize costly global memory access by maximizing on-chip (LDS/register) reuse.
- **Instruction pipeline scheduling** (software pipelining) to hide memory latency by prefetching the next K-block while computing the current block.
- **Specialization for small K (e.g. K=256)** to handle memory-bound cases differently from compute-bound cases.
- **Compiler and launch configuration tweaks** to ensure the GPU executes the kernel at optimal wavefront size, uses fast math approximations, and achieves high occupancy or the best occupancy-performance balance.

We detail each of these steps below, including the implementation approach and the rationale, and finally estimate the performance impact of each optimization.

![acb33744-23e0-4865-b98e-ba548bedeead.png](al-folio/assets/img/acb33744-23e0-4865-b98e-ba548bedeead.png)

*Figure: The AMD GCN Compute Unit (CU) architecture – each CU has four 16-lane SIMD units and a large 256 KB vector register file (VGPR) split into 64 KB per SIMD. Each thread (lane) can use up to 256 32-bit registers [olcf.ornl.gov](https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf#:~:text=%E2%81%83%204x%20SIMD%20vector%20units,byte%20entries), enabling aggressive register blocking. We leverage this architecture to optimize FP8-BF16 GEMM.*

## Register Tiling for Output C

To maximize data reuse, each thread in a HIP kernel should compute a small **tile of C** held in registers, instead of computing just one C element. By accumulating a tile of outputs per thread, we amortize the cost of loading A and B values over multiple FMA (fused multiply-add) operations. On AMD MI300, the register file is large (up to 256 FP32 registers per thread lane) [olcf.ornl.gov](https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf#:~:text=%E2%81%83%204x%20SIMD%20vector%20units,byte%20entries), which allows each thread to safely keep several partial sums and input values in registers without spilling to memory. This **register tiling** approach dramatically reduces redundant memory accesses:

- **Thread output tile size:** We choose a tile size (e.g. 4×4 or 2×8 outputs per thread) such that the required registers fit within the 256 VGPR limit. For example, a 4×4 tile of BF16 outputs requires 16 accumulators (32-bit each for accumulation in FP32), plus some registers for loaded A/B data. This is well within the per-thread budget on MI300. Using more registers (larger tile) can further increase data reuse but may reduce occupancy, so we balance tile dimensions to keep enough threads active.
- **Data reuse via outer-product:** With a tile per thread, we can use each loaded value multiple times. For instance, if a thread’s tile is 4 outputs across 4 columns of C in the same row (M), the thread can load one A value (one row, one K index) and four B values (for four columns) and multiply-accumulate them into its 4 output registers. In the next step, it loads the next A value (same row) and another set of 4 B values, accumulating into the same outputs. This effectively performs 4 FMAs per pair of loads (one A, four B’s), boosting the arithmetic intensity.
- **Warp-level tiling:** Threads are grouped in wavefronts (warps) of 64 on AMD GCN hardware [olcf.ornl.gov](https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf#:~:text=%E2%96%AA%20AMD%20GCN%20hardware%20%E2%80%98warp%E2%80%99,addressing%20is%20enabled%20by%20default). We map each warp to a larger C submatrix (e.g. 64×64 output tile per warp, divided into 64 thread-tiles of 8×1 or 4×2 each). All threads in a wave cooperate, with each thread handling a distinct portion of the output. This organization ensures that the entire warp works on a contiguous C tile, facilitating coalesced memory accesses for both A and B, as described later.

By blocking computation into per-thread tiles in registers, we minimize writing to global memory (only once, after full accumulation) and re-use each loaded operand across multiple multiply-adds. **This substantially increases arithmetic intensity**, meaning more math operations per byte of data fetched [olcf.ornl.gov](https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf#:~:text=%E2%81%83%20Access%20is%20faster%20than,All%20rights%20reserved)[rocm.blogs.amd.com](https://rocm.blogs.amd.com/software-tools-optimization/amdgcn-isa/README.html#:~:text=Registers%20are%20high%20speed%20memory,and%20data%20is%20stored%20out). As a result, the GPU spends more time doing FMA (which it can do at petaflop rates) and less time stalled on memory. Register tiling is the foundation for approaching the provided 8.63 µs–155.30 µs latency targets.

![Screenshot 2025-04-20 at 9.02.40 PM.png](al-folio/assets/img/Screenshot_2025-04-20_at_9.02.40_PM.png)

## Shared Memory Tiling and Double-Buffering

After fetching FP8 data from global memory, we stage it in **shared memory** (called *LDS* on AMD GPUs [rocm.blogs.amd.com](https://rocm.blogs.amd.com/software-tools-optimization/amdgcn-isa/README.html#:~:text=,as%20an%20explicitly%20managed%20cache)) for reuse by all threads in the block. Shared memory acts as a manually managed cache, enabling fast access to the same data from multiple threads. We employ tiling in shared memory and double-buffering to optimize usage:

- **Tiling in LDS:** We partition the K dimension into sub-tiles (e.g. K_tile = 256 or 512) that fit in shared memory. For each such tile, we load a block of matrix A (dimensions: M_tile × K_tile) and matrix B (N_tile × K_tile) into shared memory. For example, if a thread block covers 128×128 of the output (M_tile=128, N_tile=128), and we choose K_tile=256, then A_tile size is 128×256 and B_tile is 128×256 (B is effectively transposed when loaded for convenience). All threads in the block cooperate: each thread reads a few FP8 elements from global and writes them to LDS. Because we arranged global loads coalesced, each warp brings in a contiguous chunk for A or B. Once in shared memory, values can be read by any thread in the block at **LDS speed, which is much faster than global memory (though slightly slower than registers)[olcf.ornl.gov](https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf#:~:text=%E2%81%83%20Access%20is%20faster%20than,All%20rights%20reserved)**. This allows each element of A and B to be reused by dozens of FMAs without reloading from DRAM.
- **Padding to avoid bank conflicts:** AMD’s shared memory is banked (32 banks on GCN GPUs)[olcf.ornl.gov](https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf#:~:text=%E2%81%83%2032%20banks%20with%20conflict,%E2%80%93%20shared%20by%20all%20CUs). We pad the shared arrays so that threads accessing A or B simultaneously do not contend for the same bank. For instance, if each thread reads a 32-bit FP32 value (after unpack) from shared memory, addresses that are multiples of 32 words might lie in the same bank. By adding a slight padding (e.g. an extra column) in the LDS tile for A and B, we ensure aligned, conflict-free access patterns. This way, all 32 or 64 threads in a wave can read their data in parallel without serialization due to bank conflicts.
- **Double-buffering (ping-pong buffers):** To **hide global memory latency**, we use two buffers in shared memory for each matrix. While the threads are computing on the current tile (performing FMAs using data in LDS), we concurrently prefetch the next tile of A and B into the alternate buffer (if the hardware and HIP allow overlapping global memory operations with computation). The typical sequence is:
    1. Load A_tile0 and B_tile0 into shared memory (buffer 0).
    2. __syncthreads(); Compute on tile0 (all threads read from shared buffer 0, do FMAs into registers).
    3. While computing tile0, begin loading A_tile1, B_tile1 into buffer 1 (asynchronous or by carefully placing the load instructions before the compute loop for tile0 finishes).
    4. __syncthreads(); Switch to compute on tile1 (now data is in buffer 1) while prefetching next tile into buffer 0.
    5. Repeat until all K tiles are done.
    
    This overlapping effectively creates a software pipeline. Memory fetching for the next segment of K runs in parallel with arithmetic on the current segment. By the time compute on tile `n` completes, tile `n+1` is ready in LDS. This hides the latency of global memory and ensures the compute units stay busy almost continuously[olcf.ornl.gov](https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf#:~:text=%E2%81%83%20Access%20is%20faster%20than,All%20rights%20reserved).
    
- **Thread block scheduling:** We size the shared memory tiles and thread block such that the entire tile fits in LDS (MI300 offers large shared memory per CU, likely 64 KB or more). We also consider occupancy: double-buffering means at least 2× tile data is in LDS at once, but it allows us to keep the pipeline full. Even if using more shared memory per block reduces occupancy (fewer blocks resident at once), the latency hiding from double-buffering and the increased data reuse often more than compensate.

Using shared memory tiles with double-buffering dramatically reduces the required bandwidth to global memory. Each FP8 element from A or B is fetched from DRAM only once per tile, then reused from LDS many times (on the order of M_tile or N_tile times, respectively). With double-buffering, we approach ideal usage of the memory system: global memory is almost always being loaded in the background, and the compute units are nearly 100% busy doing FMA on the foreground tile. This is essential to reach the target latencies, especially for large K where memory access would otherwise be a bottleneck.

## Wave-Level Reductions with AMD DPP and Shuffles

On some GPU architectures, partial results need to be reduced across threads. In our GEMM approach, each thread is responsible for a unique C tile, so **most of the accumulation is done in registers locally**, not requiring cross-thread reduction. However, there are scenarios where wave-level reduction techniques become useful:

- If we **split the K dimension across multiple warps or thread blocks** (so-called split-K parallelism) to reduce per-thread workload, then partial sums from different warps must be added together. This could be needed if K is extremely large or if using specialized matrix instructions (MFMA) that accumulate internally.
- If a warp of 64 threads collectively computes one large output tile, they might each compute a partial sum of an output element, then combine them.

For AMD GCN GPUs, we have **wavefront (warp) size = 64 threads** by default[olcf.ornl.gov](https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf#:~:text=%E2%96%AA%20AMD%20GCN%20hardware%20%E2%80%98warp%E2%80%99,addressing%20is%20enabled%20by%20default) (RDNA-based GPUs can use wave32 or wave64). AMD provides efficient intrinsics for cross-lane communication:

- **DPP (Data Parallel Primitives):** DPP instructions allow permuting vector register values between lanes in the same wavefront with a single instruction (with patterns like rotate, swap, etc.) and optionally combining with an ALU operation. We can use DPP to implement a warp reduction in logarithmic steps. For example, to reduce a value across 64 lanes, we can use a sequence of DPP operations that shuffle values by fixed offsets (like 1,2,4,8,... positions) while performing add operations. This is done entirely in registers without using shared memory, making it very fast. Although HIP C++ does not expose DPP directly, the compiler may provide built-ins or map high-level warp shuffle calls to DPP patterns on GCN. An alternative is using inline assembly, but since we avoid that, we rely on provided intrinsics or optimize the algorithm to minimize need for these operations.
- **Wave shuffles (ballot and shuffle):** Modern HIP includes warp shuffle functions (similar to CUDA’s `__shfl_xor`, `__shfl_down`, etc.) that on AMD will utilize the GPU’s ds_swizzle or other cross-lane capabilities. Using `__shfl_down_sync` for example, threads can pass values to one another. If DPP is not directly accessible, we can still achieve reductions by iterative shuffle: e.g., each thread adds the value from a partner thread (e.g. thread `t` reads value from `t+32`, then `t+16`, etc. in a loop). Because AMD wavefront is 64, it will take 6 iterations (for 64 lanes) instead of 5 (for 32 lanes on NVIDIA), but the principle is the same.
- **SIMD-wave intrinsic optimization:** On AMD, an alternative to explicit shuffle is to use wave-wide prefix sum or reduction library calls, or even to use the SALU (scalar ALU) for uniform operations if applicable. If our partial sums to reduce are arranged such that each wave’s lanes that need to be summed are contiguous, we could have one lane compute the sum of some values by reading them from neighboring lanes via a special read-first-lane instruction or by writing to LDS and reading by one lane. However, these are generally slower than using vector ops like DPP.

In the context of our GEMM, we minimize the need for cross-thread reduction by designing the kernel so that each output element is computed by a single thread. Still, in edge cases or certain tiling strategies, wave-level reductions can sum contributions efficiently without leaving the wave. Leveraging these **AMD-friendly idioms** (especially DPP, which is known to be very efficient on GCN for reductions) can save on extra memory operations. For example, rather than each thread storing partial results to shared memory and a designated thread reading them to sum (which would be slow), a DPP add can do it in registers in one or two cycles. If our kernel needed to sum partial C from multiple phases or from multiple split-K slices, a couple of wave-level shuffles would complete the sum with negligible overhead.

In summary, while our primary strategy is to give each thread its own outputs (avoiding heavy cross-lane reductions), AMD’s wavefront collectives are available to combine partial results as needed. This ensures that any necessary reductions are done **in-register across the 64 lanes** with minimal synchronization cost.

## Memory Hierarchy Optimizations (Global → Shared → Register)

![Screenshot 2025-04-20 at 9.01.26 PM.png](al-folio/assets/img/Screenshot_2025-04-20_at_9.01.26_PM.png)

Optimizing data movement through the memory hierarchy is crucial for performance. We intentionally stage data in the **global → shared (LDS) → register** hierarchy to maximize reuse and bandwidth at each level:

- **Global Memory (HBM/DDR)**: This is the slowest (high latency) but highest capacity level. We minimize accesses here by reading each matrix element at most once. All FP8 A and B elements are read from global memory, coalesced and in vectorized chunks as described, then reused heavily from faster memories. We also ensure that global stores of the BF16 results are coalesced: each thread writes its few output values contiguously, and threads collectively write a full aligned 128B segment of C at a time to achieve full throughput.
- **L2 and L1 caches**: The MI300 GPU likely has an L2 cache and possibly a L1 data cache for global memory (16 KiB per CU as per GCN architecture)[olcf.ornl.gov](https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf#:~:text=%E2%81%83%2032%20banks%20with%20conflict,%E2%80%93%20shared%20by%20all%20CUs). Our access pattern (coalesced and streaming) will naturally use the caches effectively for any repeated accesses. In GEMM, however, each element of A and B is typically used once per kernel execution (because we tile K), so caching of global memory is less critical than tiling in shared memory. Still, the writes to C (BF16) will benefit from write-combining in cache. We also rely on the cache to handle any memory bank differences between global and LDS.
- **Shared Memory (LDS)**: This is the on-chip user-managed scratchpad that we explicitly use for blocking. Its latency is an order of magnitude lower than global memory, and it allows **many threads to access data in parallel** without congesting global memory[rocm.blogs.amd.com](https://rocm.blogs.amd.com/software-tools-optimization/amdgcn-isa/README.html#:~:text=,as%20an%20explicitly%20managed%20cache). By loading each tile into LDS, we effectively **convert random global accesses into structured on-chip accesses**. Within a tile, threads access A and B from LDS in a pattern that the compiler can schedule very efficiently (often, it will map LDS loads to on-chip operations that take only a few cycles). We take advantage of this by loading large tiles (subject to occupancy limits) so that each global load yields hundreds or thousands of on-chip accesses (each multiply using the data is an “access” to the operand).
- **Registers (VGPR/AGPR)**: The final level is the registers, which have the fastest access (single cycle). All critical innermost computations occur here. After a value is loaded from shared memory into a register, it is reused for multiple FMAs (for example, a loaded B value in a register might be multiplied by 4 different A values in succession by the same thread, producing 4 updates to outputs). The output accumulations themselves remain in registers until the end, so there is no spilling to memory mid-computation. According to AMD optimization guidelines, “almost all computation uses registers” for precisely this reason [rocm.blogs.amd.com](https://rocm.blogs.amd.com/software-tools-optimization/amdgcn-isa/README.html#:~:text=Registers%20are%20high%20speed%20memory,and%20data%20is%20stored%20out). Using registers for accumulation and operand reuse allows us to achieve the theoretical FMA throughput of the hardware.

By **orchestrating data movement** in this way, we maximize bandwidth utilization at each level: Global memory bandwidth is saved by only reading each value once; the LDS is used to serve data at high speed to many threads (and thanks to careful padding, without bank conflicts); registers are used to their fullest extent to keep data close to the ALUs. The net effect is that the kernel approaches the roofline limit: the compute units are fed with data at the rate needed to keep them busy, hitting the peak FLOPS for FP8×FP8→BF16 accumulation.

## Pipeline Scheduling to Hide Memory Latency

To achieve overlap of computation and memory transfers, we implement **software pipelining** in the kernel. The concept is to reorganize instructions so that while one set of operations is waiting on memory, another independent set of operations is executed, thereby hiding the latency. In our GEMM kernel, this is realized through the double-buffering mechanism and careful scheduling:

- **Interleaving loads and computes:** We issue asynchronous global loads for the next tile well *before* finishing the computation on the current tile. In practice, we structure the kernel as a loop over K-tiles, where each iteration does:
    1. Issue global loads for A_tile and B_tile of the *next* iteration (except in the very last iteration) and store them into the alternate shared memory buffer.
    2. Compute using the data from the *current* tile (already in shared memory from the previous iteration).
    3. Synchronize to ensure loads are completed before the next iteration.
    
    By moving the load instructions to earlier in the loop body (relative to the use of the data), we create an effective **prefetch**. The compiler will see that the loads of tile `n+1` are not dependent on the compute of tile `n`, and if we mark them with the proper `__syncthreads()` ordering, it can overlap those memory operations with the ongoing ALU operations.
    
- **Latency hiding:** The latency to fetch data from global memory (even from HBM, which might be hundreds of cycles) is thus covered by useful computation. As long as the compute on each tile takes a comparable amount of time to the load of the next tile, the global memory latency will be fully hidden. We can tune tile sizes such that this balance is achieved. For example, if computation on a 256×K tile takes ~1000 cycles and memory latency is ~400 cycles, we are safely covering the latency. If needed, we can enlarge the tile (do more computation per tile) to increase compute time, or use more threads (which increases parallel operations per tile load).
- **Prefetch depth:** We primarily prefetch one tile ahead. MI300’s large caches and multiple memory queues might even allow prefetching more than one tile ahead, but one tile double-buffering is usually sufficient. We also ensure that the compiler does not reorder the loads and compute incorrectly; the use of barriers (`__syncthreads()`) as synchronization points naturally divides the schedule into stages, preventing hazards.
- **ILP within a tile:** In addition to overlapping different tile loads, we also increase **instruction-level parallelism** within the tile’s compute. Each thread can unroll the inner loop (over K elements within the tile) and perform multiple independent FMAs before needing the next loaded value. Modern compilers and GPUs can dual-issue or hide pipeline latency by having independent instructions ready to execute. By unrolling a bit or having each thread compute multiple outputs, we create independent sequences the hardware can intermix. This means even the latency of a shared memory load or a dependent chain is partly hidden by other independent operations on the ALUs.

The result of these scheduling techniques is that both global and shared memory latencies are largely hidden under useful work. The kernel’s execution becomes **compute-bound** rather than stall-bound: as soon as one tile’s worth of FMAs are done, the next tile’s data is already available, so the work flows steadily. This is critical for hitting the sub-10µs execution times on smaller sizes and keeping the larger cases (e.g. 6144×4608×7168) close to peak utilization without pauses. Essentially, we create a steady-state pipeline from memory to ALU: at any given moment, one tile is being loaded, one is being computed, and perhaps one is being written back (for C output, though final write-back is relatively minor and can overlap too). Such overlapping can improve overall performance by a significant factor – without it, the GPU would frequently idle waiting for data.

## Specialization for Short-K Cases (K = 256)

When the K dimension is very small (256 in our benchmark cases), the problem characteristics change. With K=256, each output involves far fewer multiplications, so the computation is faster and the workload is more memory-bound (lower arithmetic intensity). To handle this efficiently, we introduce specialized strategies for **short-K scenarios**:

- **Larger thread blocks covering more of M×N:** With a small K, each thread’s computation (even for a decent tile size) is relatively short. We can afford to increase the tile sizes in M and N to involve more threads and cover a larger portion of the output matrix in one go. For example, for K=256 we might let each thread block compute a 256×256 chunk of C (as opposed to perhaps 128×128 for K=7168). This means each block will load more A and B data (which is still manageable since K is small, the total LDS usage = 256×256 FP8 for A + B, which is not huge) and produce more output. The advantage is twofold: (a) we get more reuse of each loaded A/B element (now each element might be used to produce 256 results instead of 128, improving flops/byte), and (b) we launch fewer blocks overall, reducing kernel launch and scheduling overhead.
- **Fewer tiles (or a single tile) in K dimension:** If K=256, we could even make the entire K a single tile. That is, the inner loop that iterates over K tiles becomes trivial – we load 256 once and compute. This eliminates the overhead of the loop and any double-buffering (since it’s not needed – we can load everything once). The whole operation becomes one bulk matrix multiplication of a tile with K fully unrolled. This is extremely efficient because it removes loop bookkeeping and extra synchronization. We will ensure that the code path for small K simply loads A and B into shared memory (or even directly into registers if feasible) in one shot, and then performs the FMAs in an unrolled loop.
- **Maximizing parallelism**: With fewer arithmetic operations to perform, it’s important to keep all SMs (CUs) on the GPU busy. We may increase the number of thread blocks (by using smaller blocks covering smaller M×N tiles) to occupy all compute units, even if each block finishes quickly. Essentially, the kernel launch configuration for K=256 might use more blocks with less work each, to ensure latency is minimized. However, we must balance this with the benefit of larger tiles – so there's a sweet spot. In practice, for sizes like 1024×7168×256, the M and N are large enough to have many blocks even with large tile sizes. For 6144×7168×256, we can still tile in a way that we have say 24 blocks in M and 28 in N or similar to fill the GPU.

The short-K specialization is justified by the roofline model: a smaller K means a lower computational intensity, so the kernel tends to be memory-bandwidth-limited. We adapt by **reducing overhead and enhancing parallelism**. These special-case kernels for K=256 may achieve better efficiency (in TFLOPs) than a one-size-fits-all kernel that is geared toward large K. For instance, our baseline “speed-of-light” numbers show the FP8 GEMM achieving ~1300 TFLOPs for large K, but only ~600 TFLOPs for the 256 K cases. Our specialized optimizations aim to close that gap by boosting memory efficiency (through larger tiles and one-shot loading) and ensuring full device occupancy. As a result, we expect the K=256 cases to see a significant speedup, possibly approaching the performance of large-K cases (maybe 1.5–2× faster than they would be with the generic kernel). This helps us meet or beat the 3.17 µs and 17.27 µs targets for those short-K shapes.

## Compiler Flags and Launch Configuration Tweaks

Even with a well-crafted kernel, certain compiler options and launch parameters can impact performance on MI300. We employ these tweaks to squeeze out the last bit of performance:

- **Wavefront size selection:** AMD’s RDNA and CDNA architectures can support wavefront sizes of 32 or 64. By default, GCN (like MI250) uses 64 threads per wave[olcf.ornl.gov](https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf#:~:text=%E2%96%AA%20AMD%20GCN%20hardware%20%E2%80%98warp%E2%80%99,addressing%20is%20enabled%20by%20default). We will experiment with `mwavefrontsize64` (and 32) to see which yields better performance [rocmdocs.amd.com](https://rocmdocs.amd.com/projects/llvm-project/en/latest/reference/rocmcc.html#:~:text=%60). A wavefront of 64 maximizes vector utilization (all 64 ALUs in a SIMD unit), which is generally best for dense GEMM. However, a wave32 could allow higher occupancy or better scheduling on RDNA-based GPUs. If MI300’s GPU is CDNA3 with fixed wave64, we stick to 64. But if it can use 32, we might try wave32 for the short-K kernel to increase the number of warps (thus more parallel loads) per CU. In any case, we explicitly set the wavefront size to avoid any ambiguity, and ensure our code doesn’t assume a warp of 32 (a common NVIDIA assumption)[olcf.ornl.gov](https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf#:~:text=%E2%80%A2%20Hard,register%20file%20on%20NVIDIA%20hardware).
- **Fast math and precision flags:** We compile with `O3 -ffast-math` to allow aggressive optimization of floating-point calculations [rocmdocs.amd.com](https://rocmdocs.amd.com/projects/llvm-project/en/latest/reference/rocmcc.html#:~:text=%60). Fast math will enable fused multiply-add (which we want – though BF16/FP32 FMAs are usually fused anyway) and might let the compiler re-associate operations. Since we handle scaling and conversion carefully, we ensure these transformations won’t break our algorithm. We also enable any available flags for denormal flushing (e4m3 doesn’t really have subnormals, and BF16 has limited range, so flush-to-zero is fine). Fast math can improve throughput by a few percent by using approximate math functions (not heavily used in GEMM) and by relaxing IEEE compliance, which is acceptable here.
- **Bfloat16 and FP8 support intrinsics:** We use built-in types for BF16 and potentially FP8 (if provided). For example, some compilers have `__bf16` type or `_Float16` that maps to BF16 on AMD. Using these can sometimes allow the compiler to use special hardware instructions (like packed BF16 stores or matrix multiply-accumulate units). If MI300 supports WMMA-like instructions for FP8×FP8→BF16, the compiler might auto-generate them when using specific types or intrinsics. We will ensure our code is ready to take advantage by using the appropriate data types and letting the compiler schedule those operations.
- **Occupancy tuning:** We will launch the kernel with an optimal number of threads per block (work-group size) that balances resource usage. For instance, a 256-thread block (4 warps of 64) is common for GEMM. We ensure this count divides evenly into our tile sizes. We might also use the HIP occupancy API or launch bounds attributes to make sure we don’t launch more blocks per CU than ideal. For example, if each block uses a lot of shared memory, the GPU may only allow a few blocks resident at once. We might explicitly limit to, say, 2 blocks per CU (occupancy 50%) if that avoids LDS capacity over-subscription, because having more would just cause spills or LDS thrashing. Past research has shown that highest occupancy isn’t always best for GEMM – sometimes fewer blocks (lower occupancy) but with more registers and shared memory per block yields better performance[olcf.ornl.gov](https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf#:~:text=%E2%80%A2%20Hard,register%20file%20on%20NVIDIA%20hardware). We use those insights here.
- **Unroll and pipeline pragma hints:** We can use `#pragma unroll` for the inner loop over K to ensure the compiler unrolls it sufficiently (especially for K=256 case where we might unroll fully). We can also use `__syncthreads()` placement and perhaps `__builtin_amdgcn_s_barrier()` to fine-tune synchronization points. If available, we might use the new asynchronous copy intrinsic from global to shared (HIP.maybe has something like `llvm.amdgcn.raw.buffer.load` we can invoke) to better overlap loads. All such tweaks are done at compile time without writing assembly by hand.
- **Vectorization and alignment hints:** We align arrays to 128-bit boundaries (using `__align__` keywords on shared memory buffers and appropriate pitch for global memory) to help the compiler generate coalesced accesses. We might also use the `restrict` keyword on pointers to A, B, C to help the compiler optimize loads/stores (guaranteeing no alias). Additionally, setting launch parameters so that `N` is a multiple of the warp size ensures no partial warps accessing out-of-bounds, etc.

These compiler and configuration optimizations ensure that our kernel runs at full throttle. For example, forcing wave64 and using fast-math can directly increase throughput by enabling more ILP and better FMA scheduling [rocmdocs.amd.com](https://rocmdocs.amd.com/projects/llvm-project/en/latest/reference/rocmcc.html#:~:text=%60). Proper block sizing avoids occupancy pitfalls and ensures we fully utilize the GPU’s CUs. In sum, we tune the environment such that nothing prevents the hardware from achieving the performance we expect from our algorithmic optimizations.

##
